{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09eaa8a9",
   "metadata": {},
   "source": [
    "# 05: Checkpoint 1 Review\n",
    "\n",
    "üéâ Congratulations! You've completed the foundational phase of your machine learning journey. This checkpoint consolidates everything you've learned and prepares you for the exciting world of classical ML algorithms.\n",
    "\n",
    "> üí° **Companion Reading**: This notebook pairs with [05_checkpoint1_review.md](05_checkpoint1_review.md) for comprehensive review questions, concept mapping, and deeper insights.\n",
    "\n",
    "## üéØ Review Objectives\n",
    "- Synthesize knowledge from linear algebra, probability, calculus, and Python tooling\n",
    "- Test understanding through interactive exercises and real-world applications\n",
    "- Identify connections between mathematical concepts and machine learning\n",
    "- Build confidence for Phase 02: Classical Machine Learning\n",
    "- Assess readiness through comprehensive self-evaluation\n",
    "\n",
    "## üß† Foundation Recap\n",
    "\n",
    "Let's quickly review what we've covered:\n",
    "\n",
    "**üìê Linear Algebra**: Vectors, matrices, transformations, and the geometric intuition behind ML\n",
    "**üìä Probability & Statistics**: Uncertainty quantification, distributions, and Bayesian reasoning  \n",
    "**üìà Calculus**: Derivatives, gradients, and optimization - the engine of learning\n",
    "**üêç Python Tooling**: Pandas, NumPy, matplotlib, and scikit-learn for practical ML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b1cf1",
   "metadata": {},
   "source": [
    "## üéÆ Interactive Knowledge Check\n",
    "\n",
    "Test your understanding with these interactive exercises. Try to answer *before* running each cell!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dd42d3",
   "metadata": {},
   "source": [
    "**Q1.** What does the dot product tell you about two vectors?\n",
    "- A. Their difference\n",
    "- B. Their alignment\n",
    "- C. Their angle\n",
    "- D. Whether they‚Äôre the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510906d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('‚úÖ Answer: B ‚Äî It measures alignment (via cosine of angle).')\n",
    "print('üí° Explanation: The dot product a¬∑b = |a||b|cos(Œ∏) where Œ∏ is the angle between vectors.')\n",
    "print('   - When vectors point same direction: cos(0¬∞) = 1, maximum dot product')\n",
    "print('   - When perpendicular: cos(90¬∞) = 0, dot product = 0')\n",
    "print('   - When opposite: cos(180¬∞) = -1, negative dot product')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da57f80a",
   "metadata": {},
   "source": [
    "**Q2.** What‚Äôs the key property of a probability density function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1278aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('‚úÖ Answer: The area under the curve equals 1.')\n",
    "print('üí° Explanation: This is the normalization property of probability distributions.')\n",
    "print('   - For any interval [a,b], the area gives P(a ‚â§ X ‚â§ b)')\n",
    "print('   - Total probability across all possible values must equal 1')\n",
    "print('   - This distinguishes PDFs from general functions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc82e518",
   "metadata": {},
   "source": [
    "**Q3.** What does the gradient point toward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d9f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('‚úÖ Answer: The direction of steepest increase.')\n",
    "print('üí° Explanation: The gradient ‚àáf is a vector pointing uphill.')\n",
    "print('   - In ML, we want to minimize cost, so we go opposite to gradient')\n",
    "print('   - Gradient descent: Œ∏ = Œ∏ - Œ±‚àáf(Œ∏)')\n",
    "print('   - At minimum, gradient = 0 (no more slope to follow)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927bb2fa",
   "metadata": {},
   "source": [
    "**Q4.** What does `df.describe()` return in pandas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71a6f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('‚úÖ Answer: Summary statistics for each numeric column.')\n",
    "print('üí° Explanation: Provides count, mean, std, min, 25%, 50%, 75%, max')\n",
    "print('   - Essential first step in exploratory data analysis')\n",
    "print('   - Helps identify outliers, missing values, and data distribution')\n",
    "print('   - Only works on numeric columns by default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c9890",
   "metadata": {},
   "source": [
    "## üß≠ Visual Concept Map\n",
    "Try sketching how these ideas are connected: vectors ‚Üí dot product ‚Üí matrix operations ‚Üí gradient descent ‚Üí loss functions ‚Üí model fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4ee485",
   "metadata": {},
   "source": [
    "## üß† Reflection\n",
    "Write a short answer:\n",
    "- What concept came most naturally to you?\n",
    "- What still feels confusing?\n",
    "- Where have you seen these ideas outside of ML?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f512230",
   "metadata": {},
   "source": [
    "## üß† Interview Memory Test\n",
    "These questions will test your memory of concepts, math, and Python syntax that are often asked in interviews. Try to write the answers yourself before running the solution cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90edad6b",
   "metadata": {},
   "source": [
    "**Q1.** Write a function in NumPy to compute the dot product between two vectors `a` and `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a57fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dot_product(a, b):\n",
    "    return np.dot(a, b)\n",
    "\n",
    "# Example test\n",
    "a = np.array([1, 2])\n",
    "b = np.array([3, 4])\n",
    "dot_product(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac34e72",
   "metadata": {},
   "source": [
    "**Q2.** Write out the mathematical formula for Bayes‚Äô Theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayes' Theorem\n",
    "# P(A|B) = (P(B|A) * P(A)) / P(B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec748cb",
   "metadata": {},
   "source": [
    "**Q3.** Given `f(g(x)) = (2x + 1)^2`, use the chain rule to compute the derivative `df/dx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee791e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let g(x) = 2x + 1\n",
    "# Then f(g(x)) = g(x)^2 = (2x + 1)^2\n",
    "# df/dx = 2 * (2x + 1) * 2 = 4 * (2x + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea2a8c",
   "metadata": {},
   "source": [
    "**Q4.** What does `df.describe()` return in pandas? Try it on the `tips` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b117b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79a6a1e",
   "metadata": {},
   "source": [
    "**Q5.** Describe the key steps in gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a663748",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('‚úÖ Answer: The iterative optimization algorithm with these steps:')\n",
    "print('1. Initialize parameters (e.g., weights) randomly')\n",
    "print('2. Compute the gradient of the loss function ‚àáL(Œ∏)')\n",
    "print('3. Update parameters: Œ∏ = Œ∏ - Œ±‚àáL(Œ∏) (move opposite to gradient)')\n",
    "print('4. Repeat until convergence (gradient ‚âà 0 or max iterations)')\n",
    "print('')\n",
    "print('üí° Key insight: We follow the slope downhill to find the minimum!')\n",
    "print('   - Learning rate Œ± controls step size')\n",
    "print('   - Too small Œ±: slow convergence')\n",
    "print('   - Too large Œ±: may overshoot minimum')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üîó Integrated Practical Exercise\n",
    "\n",
    "Let's combine all our knowledge in a mini machine learning project that uses concepts from every module!\n"
   ],
   "id": "51d3ed23ecab03ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic data that demonstrates our concepts\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# Linear algebra: Create feature matrix\n",
    "X = np.random.randn(n_samples, 2)  # 2D feature space\n",
    "true_weights = np.array([3, -2])   # True relationship\n",
    "noise = np.random.normal(0, 0.5, n_samples)  # Probability: Gaussian noise\n",
    "\n",
    "# Create target variable: y = X @ weights + noise\n",
    "y = X @ true_weights + noise\n",
    "\n",
    "print(\"üßÆ Linear Algebra in Action:\")\n",
    "print(f\"Feature matrix X shape: {X.shape}\")\n",
    "print(f\"True weights: {true_weights}\")\n",
    "print(f\"Matrix multiplication X @ weights creates our target relationship\")\n",
    "\n",
    "# Convert to DataFrame for pandas practice\n",
    "df = pd.DataFrame(X, columns=['feature_1', 'feature_2'])\n",
    "df['target'] = y\n",
    "\n",
    "print(f\"\\nüìä Data Analysis with Pandas:\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(df['feature_1'], df['target'], alpha=0.6)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Feature 1 vs Target')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(df['feature_2'], df['target'], alpha=0.6)\n",
    "plt.xlabel('Feature 2')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Feature 2 vs Target')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(df['target'], bins=15, alpha=0.7)\n",
    "plt.xlabel('Target Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Target Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Machine Learning with scikit-learn\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# The model learned weights (should be close to true_weights)\n",
    "learned_weights = model.coef_\n",
    "print(f\"\\nü§ñ Machine Learning Results:\")\n",
    "print(f\"True weights:    {true_weights}\")\n",
    "print(f\"Learned weights: {learned_weights}\")\n",
    "print(f\"Difference:      {np.abs(true_weights - learned_weights)}\")\n",
    "\n",
    "# Calculus connection: The model minimized mean squared error\n",
    "predictions = model.predict(X)\n",
    "mse = mean_squared_error(y, predictions)\n",
    "print(f\"\\nMean Squared Error: {mse:.4f}\")\n",
    "print(\"üí° The model used calculus (gradient descent) to minimize this error!\")\n",
    "\n",
    "print(f\"\\nüéØ Concepts Demonstrated:\")\n",
    "print(\"‚úì Linear Algebra: Matrix multiplication, vector operations\")\n",
    "print(\"‚úì Probability: Gaussian noise, statistical distributions\")  \n",
    "print(\"‚úì Calculus: Gradient descent optimization (hidden in .fit())\")\n",
    "print(\"‚úì Python Tools: NumPy arrays, pandas DataFrames, matplotlib plots, sklearn models\")"
   ],
   "id": "d018c312a7ad6377"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ‚úÖ Comprehensive Self-Assessment\n",
    "\n",
    "### üéØ Phase 1 Mastery Checklist\n",
    "\n",
    "Check off each item as you master it:\n",
    "\n",
    "#### üìê Linear Algebra\n",
    "- [ ] I understand what vectors and matrices represent geometrically\n",
    "- [ ] I can compute dot products and explain their meaning\n",
    "- [ ] I can multiply matrices and describe the transformation effect\n",
    "- [ ] I understand why matrix multiplication order matters (AB ‚â† BA)\n",
    "- [ ] I can visualize linear transformations in 2D space\n",
    "\n",
    "#### üìä Probability & Statistics  \n",
    "- [ ] I can simulate random processes and understand convergence\n",
    "- [ ] I can differentiate between discrete and continuous distributions\n",
    "- [ ] I can calculate and interpret conditional probabilities\n",
    "- [ ] I understand Bayes' theorem and can apply it to real problems\n",
    "- [ ] I can read and interpret probability distribution plots\n",
    "\n",
    "#### üìà Calculus\n",
    "- [ ] I understand derivatives as rates of change\n",
    "- [ ] I can apply the chain rule to nested functions\n",
    "- [ ] I understand gradients in multiple dimensions\n",
    "- [ ] I can explain how gradient descent works\n",
    "- [ ] I know how learning rate affects optimization\n",
    "\n",
    "#### üêç Python Tooling\n",
    "- [ ] I can load, explore, and clean datasets with pandas\n",
    "- [ ] I can create informative visualizations with matplotlib/seaborn\n",
    "- [ ] I can perform feature engineering to improve model inputs\n",
    "- [ ] I can build and evaluate models with scikit-learn\n",
    "- [ ] I understand the importance of train/test splits\n",
    "\n",
    "#### üîó Integration\n",
    "- [ ] I can connect mathematical concepts to ML applications\n",
    "- [ ] I understand how all four areas work together in ML\n",
    "- [ ] I can explain the ML workflow from data to model\n",
    "- [ ] I'm ready to tackle classical ML algorithms\n",
    "\n",
    "### üöÄ Readiness Assessment\n",
    "\n",
    "**You're ready for Phase 2 if you can:**\n",
    "1. Explain how matrix multiplication relates to neural network forward passes\n",
    "2. Describe why we need probability in machine learning\n",
    "3. Explain how gradient descent trains ML models\n",
    "4. Build a complete ML pipeline from raw data to trained model\n",
    "\n",
    "### üîó Next Steps\n",
    "- Review any unchecked items in the mastery checklist\n",
    "- Revisit companion theory files for deeper mathematical insights:\n",
    "  - [01_linear_algebra_intro.md](../01_linear_algebra_intro/01_linear_algebra_intro.md)\n",
    "  - [02_probability_statistics.md](../02_probability_statistics/02_probability_statistics.md)\n",
    "  - [03_calculus_for_ml.md](../03_calculus_for_ml/03_calculus_for_ml.md)\n",
    "  - [04_python_tooling.md](../04_python_tooling/04_python_tooling.md)\n",
    "- Practice with additional datasets and problems\n",
    "- **Begin Phase 02: Classical Machine Learning!**\n",
    "\n",
    "### üí° Key Takeaways\n",
    "- **Mathematics is the Language of ML**: Linear algebra, probability, and calculus provide the foundation\n",
    "- **Python Tools Enable Practice**: Theory becomes reality through hands-on coding\n",
    "- **Everything Connects**: Each concept builds on and reinforces the others\n",
    "- **Visualization Builds Intuition**: Always try to see what's happening geometrically\n",
    "- **Practice Makes Perfect**: The more you apply these concepts, the more natural they become\n",
    "\n",
    "### üéâ Congratulations!\n",
    "You've built a solid mathematical and practical foundation for machine learning. These concepts will appear everywhere in your ML journey - from simple linear regression to complex neural networks. You're now ready to see how these foundations enable powerful learning algorithms!\n",
    "\n",
    "**Next stop: Phase 02 - Classical Machine Learning** üöÄ\n"
   ],
   "id": "ef23aaf1d924cc7a"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
