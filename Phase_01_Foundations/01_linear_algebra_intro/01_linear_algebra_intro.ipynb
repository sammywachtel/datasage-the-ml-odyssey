{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 01: Linear Algebra Intro\n",
    "\n",
    "Welcome to your first deep dive! Today weâ€™ll build intuition and hands-on experience with vectors, matrices, and their operationsâ€”the foundation of almost every ML algorithm.\n",
    "\n",
    "> ðŸ’¡ **Companion Reading**: This notebook pairs with [01_linear_algebra_intro.md](01_linear_algebra_intro.md) for deeper mathematical insights, analogies, and tutor guidance.\n",
    "\n",
    "## ðŸŽ¯ Objectives\n",
    "- Understand what vectors and matrices are conceptually and computationally\n",
    "- Learn how to compute dot products and matrix multiplication\n",
    "- Visualize geometric interpretations of linear transformations\n",
    "- Explore how these concepts relate to ML models\n",
    "- Build intuition through hands-on coding and visualization\n"
   ],
   "id": "3a1723715fb88362"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸŽ¯ Understanding Vectors: The Foundation of Machine Learning\n",
    "\n",
    "Before we dive into operations, let's build a deep understanding of **what vectors are** and **why they're absolutely central** to machine learning.\n",
    "\n",
    "### What Are Vectors?\n",
    "\n",
    "A **vector** is much more than just \"a list of numbers.\" It's a mathematical object that represents:\n",
    "- **Magnitude** (how big/long it is)\n",
    "- **Direction** (which way it points)\n",
    "- **Position in space** (where it lives in multi-dimensional space)\n",
    "\n",
    "Think of vectors as **arrows in space** that encode information about data, relationships, and transformations.\n",
    "\n",
    "### Why Vectors Are Central to Machine Learning\n",
    "\n",
    "**Every piece of data in ML becomes a vector:**\n",
    "- ðŸ“¸ **Images**: Each pixel's color values â†’ vector of intensities\n",
    "- ðŸ“ **Text**: Word frequencies or embeddings â†’ vector representations  \n",
    "- ðŸ‘¤ **User profiles**: Age, income, preferences â†’ feature vectors\n",
    "- ðŸŽµ **Audio**: Frequency components over time â†’ signal vectors\n",
    "- ðŸ§¬ **DNA**: Nucleotide sequences â†’ biological feature vectors\n",
    "\n",
    "**All ML algorithms work by:**\n",
    "1. **Representing** data as vectors in high-dimensional spaces\n",
    "2. **Measuring** relationships between vectors (similarity, distance)\n",
    "3. **Transforming** vectors to find patterns and make predictions\n",
    "4. **Learning** optimal vector transformations from data\n",
    "\n",
    "Let's explore this with concrete examples!\n"
   ],
   "id": "3b49c5e9d7bcdd4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example 1: Representing different types of data as vectors\n",
    "print(\"=== DATA AS VECTORS ===\")\n",
    "\n",
    "# A simple user profile as a vector [age, income_k, hours_online_per_day]\n",
    "user1 = np.array([25, 45, 3])  # 25 years old, $45k income, 3 hours online\n",
    "user2 = np.array([35, 75, 1])  # 35 years old, $75k income, 1 hour online\n",
    "user3 = np.array([22, 35, 8])  # 22 years old, $35k income, 8 hours online\n",
    "\n",
    "print(\"User profiles as vectors:\")\n",
    "print(f\"User 1: {user1} (age, income_k, hours_online)\")\n",
    "print(f\"User 2: {user2}\")\n",
    "print(f\"User 3: {user3}\")\n",
    "\n",
    "# Which users are most similar? We can measure this with vector similarity!\n",
    "similarity_1_2 = np.dot(user1, user2) / (np.linalg.norm(user1) * np.linalg.norm(user2))\n",
    "similarity_1_3 = np.dot(user1, user3) / (np.linalg.norm(user1) * np.linalg.norm(user3))\n",
    "\n",
    "print(f\"\\nSimilarity between User 1 and User 2: {similarity_1_2:.3f}\")\n",
    "print(f\"Similarity between User 1 and User 3: {similarity_1_3:.3f}\")\n",
    "print(\"Higher values mean more similar users!\")\n"
   ],
   "id": "2c89547928944523"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Visual Understanding: Vectors as Arrows\n",
    "\n",
    "Let's visualize vectors to build geometric intuition:\n"
   ],
   "id": "8bf81d37f9b100bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a comprehensive vector visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Basic vector representation\n",
    "v1 = np.array([3, 4])\n",
    "v2 = np.array([1, 3])\n",
    "ax1.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='blue', width=0.01, label=f'v1 = {v1}')\n",
    "ax1.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='red', width=0.01, label=f'v2 = {v2}')\n",
    "ax1.set_xlim(-1, 5)\n",
    "ax1.set_ylim(-1, 5)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "ax1.set_title('Vectors as Arrows in Space')\n",
    "ax1.set_xlabel('X dimension')\n",
    "ax1.set_ylabel('Y dimension')\n",
    "\n",
    "# Add magnitude annotations\n",
    "ax1.annotate(f'|v1| = {np.linalg.norm(v1):.1f}', xy=(1.5, 2), fontsize=10)\n",
    "ax1.annotate(f'|v2| = {np.linalg.norm(v2):.1f}', xy=(0.5, 1.5), fontsize=10)\n",
    "\n",
    "# Plot 2: Angles between vectors\n",
    "angle = np.arccos(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "ax2.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='blue', width=0.01, label='v1')\n",
    "ax2.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='red', width=0.01, label='v2')\n",
    "\n",
    "# Draw angle arc\n",
    "theta = np.linspace(0, angle, 50)\n",
    "r = 0.8\n",
    "ax2.plot(r * np.cos(theta), r * np.sin(theta), 'green', linewidth=2)\n",
    "ax2.text(0.4, 0.2, f'Î¸ = {np.degrees(angle):.1f}Â°', fontsize=12, color='green')\n",
    "\n",
    "ax2.set_xlim(-1, 5)\n",
    "ax2.set_ylim(-1, 5)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "ax2.set_title('Angle Between Vectors')\n",
    "ax2.set_xlabel('X dimension')\n",
    "ax2.set_ylabel('Y dimension')\n",
    "\n",
    "# Plot 3: Different vector relationships\n",
    "vectors = {\n",
    "    'Same direction': ([2, 1], [4, 2], 'green'),\n",
    "    'Perpendicular': ([1, 0], [0, 1], 'orange'),\n",
    "    'Opposite': ([1, 1], [-1, -1], 'purple')\n",
    "}\n",
    "\n",
    "colors = ['green', 'orange', 'purple']\n",
    "for i, (label, (v_a, v_b, color)) in enumerate(vectors.items()):\n",
    "    ax3.quiver(0, 0, v_a[0], v_a[1], angles='xy', scale_units='xy', scale=1, \n",
    "               color=color, width=0.008, alpha=0.7)\n",
    "    ax3.quiver(0, 0, v_b[0], v_b[1], angles='xy', scale_units='xy', scale=1, \n",
    "               color=color, width=0.008, alpha=0.7, linestyle='--')\n",
    "\n",
    "    # Calculate and display dot product\n",
    "    dot_prod = np.dot(v_a, v_b)\n",
    "    ax3.text(-2, 2-i*0.5, f'{label}: dot = {dot_prod}', color=color, fontsize=10)\n",
    "\n",
    "ax3.set_xlim(-2, 5)\n",
    "ax3.set_ylim(-2, 3)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_title('Vector Relationships & Dot Products')\n",
    "ax3.set_xlabel('X dimension')\n",
    "ax3.set_ylabel('Y dimension')\n",
    "\n",
    "# Plot 4: ML Application - Document similarity\n",
    "# Simulate word frequency vectors for documents\n",
    "doc1 = np.array([3, 1, 0, 2])  # [freq_of_'machine', 'learning', 'cooking', 'algorithm']\n",
    "doc2 = np.array([2, 2, 0, 1])  # Similar document about ML\n",
    "doc3 = np.array([0, 0, 4, 1])  # Document about cooking\n",
    "\n",
    "# Calculate similarities\n",
    "sim_1_2 = np.dot(doc1, doc2) / (np.linalg.norm(doc1) * np.linalg.norm(doc2))\n",
    "sim_1_3 = np.dot(doc1, doc3) / (np.linalg.norm(doc1) * np.linalg.norm(doc3))\n",
    "\n",
    "# Visualize in 2D projection (first two dimensions)\n",
    "ax4.scatter(doc1[0], doc1[1], s=100, color='blue', label='ML Doc 1')\n",
    "ax4.scatter(doc2[0], doc2[1], s=100, color='green', label='ML Doc 2')\n",
    "ax4.scatter(doc3[0], doc3[1], s=100, color='red', label='Cooking Doc')\n",
    "\n",
    "# Draw similarity lines\n",
    "ax4.plot([doc1[0], doc2[0]], [doc1[1], doc2[1]], 'green', alpha=0.5, linewidth=2)\n",
    "ax4.plot([doc1[0], doc3[0]], [doc1[1], doc3[1]], 'red', alpha=0.5, linewidth=2)\n",
    "\n",
    "ax4.text(1, 3, f'ML docs similarity: {sim_1_2:.3f}', fontsize=10)\n",
    "ax4.text(1, 2.5, f'ML-Cooking similarity: {sim_1_3:.3f}', fontsize=10)\n",
    "\n",
    "ax4.set_xlim(-0.5, 4.5)\n",
    "ax4.set_ylim(-0.5, 4.5)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.legend()\n",
    "ax4.set_title('ML Application: Document Similarity')\n",
    "ax4.set_xlabel('Frequency of \"machine\"')\n",
    "ax4.set_ylabel('Frequency of \"learning\"')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== KEY INSIGHTS ===\")\n",
    "print(\"â€¢ Vectors encode both magnitude (length) and direction\")\n",
    "print(\"â€¢ Angle between vectors measures their similarity/relationship\")\n",
    "print(\"â€¢ Small angles = high similarity, large angles = low similarity\")\n",
    "print(\"â€¢ ML algorithms use these geometric relationships to find patterns!\")\n"
   ],
   "id": "ceb7d000ad091305"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ“Œ Vectors and Basic Operations\n",
    "Letâ€™s start by defining vectors and doing some operations."
   ],
   "id": "b96238374ef021d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define two vectors\n",
    "v1 = np.array([1, 2])\n",
    "v2 = np.array([3, 4])\n",
    "\n",
    "# Vector addition and scalar multiplication\n",
    "print(\"v1 + v2 =\", v1 + v2)\n",
    "print(\"2 * v1 =\", 2 * v1)"
   ],
   "id": "d5b2577040af2351",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ”¢ Dot Product\n",
    "The dot product tells us how aligned two vectors are. Mathematically: **a** Â· **b** = ||a|| ||b|| cos(Î¸)\n",
    "\n",
    "**Breaking down this notation:**\n",
    "- **a** Â· **b**: The dot product of vectors **a** and **b** (read as \"a dot b\")\n",
    "- **||a||**: The magnitude (length) of vector **a** (double pipes mean \"length of\")\n",
    "- **||b||**: The magnitude (length) of vector **b**\n",
    "- **cos(Î¸)**: Cosine of the angle Î¸ (theta) between the vectors\n",
    "- **Î¸**: Greek letter representing the angle between vectors **a** and **b**\n",
    "\n",
    "This means:\n",
    "- When vectors point in the same direction (Î¸ = 0Â°): cos(0Â°) = 1, maximum dot product\n",
    "- When vectors are perpendicular (Î¸ = 90Â°): cos(90Â°) = 0, dot product = 0\n",
    "- When vectors point in opposite directions (Î¸ = 180Â°): cos(180Â°) = -1, negative dot product\n"
   ],
   "id": "8dd5e0c7c04aa55b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute dot product\n",
    "dot = np.dot(v1, v2)\n",
    "print(\"Dot product of v1 and v2:\", dot)\n",
    "\n",
    "# Let's explore the geometric meaning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the vectors\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='blue', label='v1', width=0.005)\n",
    "plt.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='red', label='v2', width=0.005)\n",
    "\n",
    "# Calculate angle between vectors\n",
    "angle = np.arccos(dot / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "print(f\"Angle between vectors: {np.degrees(angle):.1f} degrees\")\n",
    "\n",
    "plt.xlim(-1, 5)\n",
    "plt.ylim(-1, 5)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title(f'Vectors v1 and v2 (dot product = {dot})')\n",
    "plt.show()\n",
    "\n",
    "# Special case: dot product with itself gives squared magnitude\n",
    "v1_squared = np.dot(v1, v1)\n",
    "v1_magnitude_squared = np.linalg.norm(v1)**2\n",
    "print(f\"v1 Â· v1 = {v1_squared}\")\n",
    "print(f\"||v1||Â² = {v1_magnitude_squared}\")\n",
    "print(\"They're equal! This is always true.\")\n"
   ],
   "id": "6183e1f22c200bca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Quiz:**\n",
    "What is the geometric meaning of a dot product?\n",
    "- A. The angle between vectors\n",
    "- B. The projection of one vector onto another  \n",
    "- C. A measure of similarity/alignment\n",
    "- D. All of the above\n",
    "\n",
    "> **Answer**: D. All of the above! The dot product encodes the angle between vectors, can be used to compute projections, and serves as a similarity measure.\n"
   ],
   "id": "a225e0a5f11af4fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ“ Vector Projection: Hands-On Practice\n",
    "\n",
    "Now let's explore **vector projection** - one of the most important concepts in linear algebra and ML! \n",
    "\n",
    "Vector projection answers: \"How much of vector **a** lies in the direction of vector **b**?\"\n",
    "\n",
    "Think of it as the \"shadow\" that vector **a** casts onto vector **b** when light shines perpendicular to **b**.\n"
   ],
   "id": "624cc0cb0c55051a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Let's compute and visualize vector projections\n",
    "def project_vector(a, b):\n",
    "    \"\"\"Project vector a onto vector b\"\"\"\n",
    "    # Scalar projection (length of the shadow)\n",
    "    scalar_proj = np.dot(a, b) / np.linalg.norm(b)\n",
    "\n",
    "    # Vector projection (the actual shadow vector)\n",
    "    vector_proj = (np.dot(a, b) / np.dot(b, b)) * b\n",
    "\n",
    "    return scalar_proj, vector_proj\n",
    "\n",
    "# Example vectors\n",
    "a = np.array([3, 4])  # Our familiar vector from the theory!\n",
    "b = np.array([1, 0])  # Unit vector along x-axis\n",
    "\n",
    "scalar_proj, vector_proj = project_vector(a, b)\n",
    "\n",
    "print(\"=== VECTOR PROJECTION EXAMPLE ===\")\n",
    "print(f\"Vector a: {a}\")\n",
    "print(f\"Vector b: {b}\")\n",
    "print(f\"Scalar projection (length): {scalar_proj:.2f}\")\n",
    "print(f\"Vector projection: {vector_proj}\")\n",
    "\n",
    "# Let's verify our understanding with the [3,4] example from theory\n",
    "print(f\"\\nFor vector [3,4]:\")\n",
    "print(f\"Magnitude = âˆš(3Â² + 4Â²) = âˆš(9 + 16) = âˆš25 = {np.linalg.norm(a)}\")\n",
    "print(f\"Direction = arctan(4/3) = {np.degrees(np.arctan(4/3)):.1f}Â°\")\n"
   ],
   "id": "eab669fe64e6814e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualize the projection\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot original vectors\n",
    "plt.quiver(0, 0, a[0], a[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='blue', width=0.01, label=f'Vector a = {a}')\n",
    "plt.quiver(0, 0, b[0], b[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='red', width=0.01, label=f'Vector b = {b}')\n",
    "\n",
    "# Plot the projection\n",
    "plt.quiver(0, 0, vector_proj[0], vector_proj[1], angles='xy', scale_units='xy', scale=1, \n",
    "           color='green', width=0.01, label=f'Projection of a onto b')\n",
    "\n",
    "# Draw the \"shadow\" line (perpendicular from a to its projection)\n",
    "plt.plot([a[0], vector_proj[0]], [a[1], vector_proj[1]], \n",
    "         'gray', linestyle='--', alpha=0.7, label='Perpendicular drop')\n",
    "\n",
    "# Add annotations\n",
    "plt.annotate(f'Length = {np.linalg.norm(a):.1f}', \n",
    "             xy=(a[0]/2, a[1]/2 + 0.2), fontsize=12, color='blue')\n",
    "plt.annotate(f'Projection length = {scalar_proj:.1f}', \n",
    "             xy=(vector_proj[0]/2, -0.3), fontsize=12, color='green')\n",
    "\n",
    "plt.xlim(-0.5, 4)\n",
    "plt.ylim(-0.5, 4.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.title('Vector Projection: The \"Shadow\" of Vector a onto Vector b')\n",
    "plt.xlabel('X dimension')\n",
    "plt.ylabel('Y dimension')\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()\n"
   ],
   "id": "eeff6dab1917eda5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Let's explore different projection scenarios\n",
    "scenarios = [\n",
    "    (\"Same direction\", [2, 1], [4, 2]),\n",
    "    (\"Perpendicular\", [1, 0], [0, 1]),\n",
    "    (\"Opposite direction\", [1, 1], [-2, -2]),\n",
    "    (\"Acute angle\", [3, 4], [1, 1])\n",
    "]\n",
    "\n",
    "print(\"=== PROJECTION SCENARIOS ===\")\n",
    "for name, vec_a, vec_b in scenarios:\n",
    "    scalar_proj, vector_proj = project_vector(np.array(vec_a), np.array(vec_b))\n",
    "    dot_product = np.dot(vec_a, vec_b)\n",
    "\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  a = {vec_a}, b = {vec_b}\")\n",
    "    print(f\"  Dot product = {dot_product}\")\n",
    "    print(f\"  Scalar projection = {scalar_proj:.2f}\")\n",
    "    print(f\"  Vector projection = [{vector_proj[0]:.2f}, {vector_proj[1]:.2f}]\")\n"
   ],
   "id": "6387ea1305c2d271"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ML Application: Feature extraction using projection\n",
    "print(\"\\n=== ML APPLICATION: FEATURE EXTRACTION ===\")\n",
    "\n",
    "# Simulate some 2D data points (could be customer features, image features, etc.)\n",
    "data_points = np.array([\n",
    "    [2, 3],   # Customer 1: [age_normalized, income_normalized]\n",
    "    [1, 4],   # Customer 2\n",
    "    [3, 2],   # Customer 3\n",
    "    [4, 1],   # Customer 4\n",
    "    [1, 1]    # Customer 5\n",
    "])\n",
    "\n",
    "# Define a \"direction of interest\" (could be discovered by PCA, domain knowledge, etc.)\n",
    "direction = np.array([1, 1])  # Equal weight to age and income\n",
    "direction = direction / np.linalg.norm(direction)  # Normalize\n",
    "\n",
    "print(f\"Data points (customers): \\n{data_points}\")\n",
    "print(f\"Direction of interest: {direction}\")\n",
    "\n",
    "# Project all data points onto this direction\n",
    "projections = []\n",
    "for point in data_points:\n",
    "    scalar_proj, vector_proj = project_vector(point, direction)\n",
    "    projections.append(scalar_proj)\n",
    "\n",
    "projections = np.array(projections)\n",
    "print(f\"Projected values (1D features): {projections}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot original 2D data\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(data_points[:, 0], data_points[:, 1], c='blue', s=100)\n",
    "plt.quiver(0, 0, direction[0]*3, direction[1]*3, angles='xy', scale_units='xy', scale=1, \n",
    "           color='red', width=0.01, label='Projection direction')\n",
    "for i, point in enumerate(data_points):\n",
    "    plt.annotate(f'C{i+1}', xy=point, xytext=(5, 5), textcoords='offset points')\n",
    "plt.xlim(-0.5, 5)\n",
    "plt.ylim(-0.5, 5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.title('Original 2D Data')\n",
    "plt.xlabel('Feature 1 (e.g., Age)')\n",
    "plt.ylabel('Feature 2 (e.g., Income)')\n",
    "\n",
    "# Plot projected 1D data\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(projections, np.zeros_like(projections), c='green', s=100)\n",
    "for i, proj in enumerate(projections):\n",
    "    plt.annotate(f'C{i+1}', xy=(proj, 0), xytext=(0, 10), textcoords='offset points')\n",
    "plt.ylim(-0.5, 0.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.title('Projected 1D Data')\n",
    "plt.xlabel('Projected Feature Value')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== KEY INSIGHTS ===\")\n",
    "print(\"â€¢ Projection reduces dimensionality while preserving important information\")\n",
    "print(\"â€¢ The projection direction determines what aspects of data we emphasize\")\n",
    "print(\"â€¢ This is the foundation of techniques like PCA (Principal Component Analysis)\")\n",
    "print(\"â€¢ In ML, we often project high-dimensional data to lower dimensions for visualization and efficiency\")\n"
   ],
   "id": "109cc84ea1f5aebd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ”„ Matrix Multiplication\n",
    "\n",
    "Matrix multiplication combines transformations. Remember: **order matters!** AB â‰  BA in general.\n",
    "\n",
    "**Notation:** â‰  means \"not equal to\"\n",
    "\n",
    "When we multiply an mÃ—n matrix A by an nÃ—p matrix B, we get an mÃ—p matrix C.\n"
   ],
   "id": "2bcdc64a18345a44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# Multiply matrices\n",
    "C = np.dot(A, B)\n",
    "print(\"A @ B =\\n\", C)\n",
    "\n",
    "# Let's verify that order matters\n",
    "C_reverse = np.dot(B, A)\n",
    "print(\"\\nB @ A =\\n\", C_reverse)\n",
    "print(f\"\\nAre they equal? {np.array_equal(C, C_reverse)}\")\n",
    "\n",
    "# Let's understand what matrix multiplication does geometrically\n",
    "# A matrix transforms vectors - let's see how\n",
    "test_vector = np.array([1, 0])  # Unit vector along x-axis\n",
    "transformed = A @ test_vector\n",
    "print(f\"\\nOriginal vector: {test_vector}\")\n",
    "print(f\"After transformation by A: {transformed}\")\n",
    "\n",
    "# The identity matrix does nothing (like multiplying by 1)\n",
    "I = np.eye(2)  # 2x2 identity matrix\n",
    "print(f\"\\nIdentity matrix:\\n{I}\")\n",
    "print(f\"I @ test_vector = {I @ test_vector}\")  # Should be unchanged"
   ],
   "id": "2fcc9852121061d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ðŸ§  Visualizing Transformations",
   "id": "cd238c2bfab90c47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_transform(A, title=\"Transformation\"):\n",
    "    grid = np.array([[x, y] for x in range(-2, 3) for y in range(-2, 3)])\n",
    "    transformed = grid @ A.T\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.quiver(grid[:, 0], grid[:, 1], transformed[:, 0] - grid[:, 0], transformed[:, 1] - grid[:, 1], angles='xy', scale_units='xy', scale=1, color='r')\n",
    "    plt.scatter(grid[:, 0], grid[:, 1], color='blue')\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.axhline(0, color='gray', lw=1)\n",
    "    plt.axvline(0, color='gray', lw=1)\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.show()\n",
    "\n",
    "plot_transform(np.array([[2, 0], [0, 1]]), title=\"Horizontal Stretch\")"
   ],
   "id": "103e9144277393ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## âœ… Summary Quiz & Checklist\n",
    "\n",
    "### Quiz Questions\n",
    "1. **What does matrix multiplication represent geometrically?**\n",
    "   > Matrix multiplication represents the composition of linear transformations. Each matrix transforms space in some way (stretch, rotate, reflect, etc.), and multiplying matrices combines these transformations.\n",
    "\n",
    "2. **What happens when you dot a vector with itself?**\n",
    "   > You get the squared magnitude (length) of the vector: **v** Â· **v** = ||**v**||Â²\n",
    "\n",
    "3. **Which operations preserve direction?**\n",
    "   > Scalar multiplication by positive numbers preserves direction. Matrix transformations may or may not preserve direction depending on the matrix.\n",
    "\n",
    "4. **Why does AB â‰  BA in general?**\n",
    "   > Because matrix multiplication represents composition of transformations, and the order of transformations matters. Rotating then stretching gives a different result than stretching then rotating.\n",
    "\n",
    "### Self-Assessment Checklist\n",
    "Check off each item as you master it:\n",
    "\n",
    "**Vector Fundamentals:**\n",
    "- [ ] I understand what vectors are: mathematical objects with magnitude and direction\n",
    "- [ ] I can explain why vectors are central to machine learning (data representation, similarity, transformations)\n",
    "- [ ] I can give concrete examples of how different data types become vectors\n",
    "- [ ] I can visualize vectors as arrows in space and interpret their geometric meaning\n",
    "\n",
    "**Vector Relationships:**\n",
    "- [ ] I understand what angles between vectors represent (similarity/alignment)\n",
    "- [ ] I can explain why small angles mean high similarity and large angles mean low similarity\n",
    "- [ ] I can compute and interpret dot products as measures of vector alignment\n",
    "- [ ] I can connect vector similarity to real ML applications (recommendations, document analysis, etc.)\n",
    "\n",
    "**Mathematical Operations:**\n",
    "- [ ] I can compute a dot product and explain its geometric meaning\n",
    "- [ ] I can multiply two matrices and describe the geometric effect\n",
    "- [ ] I can visualize matrix transformations on a 2D grid\n",
    "- [ ] I can explain why AB â‰  BA (order matters in matrix multiplication)\n",
    "- [ ] I understand the role of the identity matrix\n",
    "\n",
    "**ML Connections:**\n",
    "- [ ] I can connect these concepts to machine learning applications\n",
    "- [ ] I understand how vectors represent data points in feature spaces\n",
    "- [ ] I can explain how ML algorithms use vector operations to find patterns\n",
    "\n",
    "### ðŸ”— Next Steps\n",
    "- Review the [companion theory file](01_linear_algebra_intro.md) for deeper mathematical insights\n",
    "- Practice with different transformation matrices\n",
    "- Think about how these concepts might apply to neural networks (hint: they're everywhere!)\n",
    "\n",
    "### ðŸ’¡ Key Takeaways\n",
    "- **Vectors**: Quantities with direction and magnitude\n",
    "- **Matrices**: Functions that transform space\n",
    "- **Dot Product**: Measures alignment between vectors\n",
    "- **Matrix Multiplication**: Combines transformations (order matters!)\n",
    "- **Geometric Intuition**: Always try to visualize what's happening in space\n"
   ],
   "id": "b8ae7582461d37c2"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
