{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddfba09c",
   "metadata": {},
   "source": [
    "# 03: Calculus for Machine Learning\n",
    "In this lesson, we‚Äôll explore the calculus behind how machine learning models learn ‚Äî especially via optimization techniques like gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91415d3c",
   "metadata": {},
   "source": [
    "## üéØ Objectives\n",
    "- Understand derivatives and gradients\n",
    "- Apply the chain rule to multivariable functions\n",
    "- Use calculus to derive loss functions\n",
    "- Visualize gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6436feb",
   "metadata": {},
   "source": [
    "## üìê Derivatives & Cost Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec3347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a quadratic cost function\n",
    "def f(x): return (x - 3)**2\n",
    "def df(x): return 2 * (x - 3)\n",
    "\n",
    "x = np.linspace(-1, 7, 100)\n",
    "y = f(x)\n",
    "dy = df(x)\n",
    "\n",
    "plt.plot(x, y, label='f(x)')\n",
    "plt.plot(x, dy, label=\"f'(x)\", linestyle='--')\n",
    "plt.axvline(3, color='gray', linestyle=':')\n",
    "plt.legend()\n",
    "plt.title(\"Function and Derivative\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c764f5",
   "metadata": {},
   "source": [
    "## üîó Chain Rule in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f9f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: f(g(x)) = (2x + 1)^2\n",
    "# df/dx = df/dg * dg/dx\n",
    "x = 2\n",
    "g = 2*x + 1\n",
    "f = g**2\n",
    "df_dx = 2 * g * 2\n",
    "print(\"df/dx at x=2:\", df_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d347eb9",
   "metadata": {},
   "source": [
    "## üßó Gradient Descent Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34800fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple gradient descent\n",
    "x = 0.0\n",
    "learning_rate = 0.1\n",
    "history = []\n",
    "\n",
    "for _ in range(20):\n",
    "    grad = df(x)\n",
    "    x -= learning_rate * grad\n",
    "    history.append(x)\n",
    "\n",
    "plt.plot(history, marker='o')\n",
    "plt.title(\"Gradient Descent Progression\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"x value\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1decc8a8",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary Quiz\n",
    "1. What does a derivative represent?\n",
    "2. What does the gradient tell us in multiple dimensions?\n",
    "3. Why is the learning rate important in gradient descent?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
