{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddfba09c",
   "metadata": {},
   "source": [
    "# 03: Calculus for Machine Learning\n",
    "\n",
    "Welcome to the mathematical engine of machine learning! Today we'll explore how calculus powers the learning process in ML models through optimization and gradient-based methods.\n",
    "\n",
    "> 💡 **Companion Reading**: This notebook pairs with [03_calculus_for_ml.md](03_calculus_for_ml.md) for deeper mathematical insights, analogies, and tutor guidance.\n",
    "\n",
    "## 🎯 Objectives\n",
    "- Understand derivatives and gradients as tools for measuring change\n",
    "- Apply the chain rule to multivariable functions (essential for neural networks)\n",
    "- Visualize and implement gradient descent optimization\n",
    "- Connect calculus concepts to real machine learning applications\n",
    "- Build intuition for how models \"learn\" through mathematical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91415d3c",
   "metadata": {},
   "source": [
    "## 📐 Derivatives & Cost Functions\n",
    "\n",
    "The derivative tells us how fast a function is changing at any point. In machine learning, we use derivatives to find the minimum of cost functions - the point where our model performs best.\n",
    "\n",
    "**Key Insight**: The derivative is zero at the minimum of a function!\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c6436feb",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a quadratic cost function (like mean squared error)\n",
    "def f(x): return (x - 3)**2\n",
    "def df(x): return 2 * (x - 3)\n",
    "\n",
    "x = np.linspace(-1, 7, 100)\n",
    "y = f(x)\n",
    "dy = df(x)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, y, 'b-', linewidth=2, label='f(x) = (x-3)²')\n",
    "plt.axvline(3, color='red', linestyle=':', label='Minimum at x=3')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title(\"Cost Function\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, dy, 'r--', linewidth=2, label=\"f'(x) = 2(x-3)\")\n",
    "plt.axhline(0, color='gray', linestyle='-', alpha=0.5)\n",
    "plt.axvline(3, color='red', linestyle=':', label='Derivative = 0')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"f'(x)\")\n",
    "plt.title(\"Derivative (Slope)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Observations:\")\n",
    "print(\"- The function has its minimum at x = 3\")\n",
    "print(\"- The derivative equals zero at the minimum\")\n",
    "print(\"- Negative derivative means function is decreasing\")\n",
    "print(\"- Positive derivative means function is increasing\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4ec3347b",
   "metadata": {},
   "source": [
    "## 🔗 Chain Rule in Action\n",
    "\n",
    "The chain rule is crucial for neural networks! It tells us how to find derivatives of nested functions - exactly what we need when signals pass through multiple layers.\n",
    "\n",
    "**Formula**: If f(g(x)), then df/dx = df/dg × dg/dx\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "48c764f5",
   "metadata": {},
   "source": [
    "# Example: f(g(x)) = (2x + 1)^2\n",
    "# This is like a simple neural network: input → linear transformation → activation\n",
    "\n",
    "def g(x):\n",
    "    \"\"\"Inner function: linear transformation\"\"\"\n",
    "    return 2*x + 1\n",
    "\n",
    "def f(g_val):\n",
    "    \"\"\"Outer function: square activation\"\"\"\n",
    "    return g_val**2\n",
    "\n",
    "def df_dg(g_val):\n",
    "    \"\"\"Derivative of outer function\"\"\"\n",
    "    return 2 * g_val\n",
    "\n",
    "def dg_dx(x):\n",
    "    \"\"\"Derivative of inner function\"\"\"\n",
    "    return 2\n",
    "\n",
    "# Chain rule in action\n",
    "x_val = 2\n",
    "g_val = g(x_val)\n",
    "f_val = f(g_val)\n",
    "\n",
    "# Manual chain rule calculation\n",
    "df_dx_manual = df_dg(g_val) * dg_dx(x_val)\n",
    "\n",
    "print(f\"At x = {x_val}:\")\n",
    "print(f\"g(x) = 2x + 1 = {g_val}\")\n",
    "print(f\"f(g(x)) = g² = {f_val}\")\n",
    "print(f\"df/dg = 2g = {df_dg(g_val)}\")\n",
    "print(f\"dg/dx = 2 = {dg_dx(x_val)}\")\n",
    "print(f\"df/dx = df/dg × dg/dx = {df_dx_manual}\")\n",
    "\n",
    "# Verify with direct calculation\n",
    "print(f\"\\nVerification: f(x) = (2x + 1)² = {(2*x_val + 1)**2}\")\n",
    "print(f\"Direct derivative: df/dx = 4(2x + 1) = {4*(2*x_val + 1)}\")\n",
    "print(\"✅ Chain rule matches direct calculation!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "357f9f05",
   "metadata": {},
   "source": [
    "## 🧗 Gradient Descent Demo\n",
    "\n",
    "Gradient descent is the workhorse of machine learning! It uses derivatives to iteratively find the minimum of a function - exactly how neural networks learn.\n",
    "\n",
    "**Algorithm**: \n",
    "1. Start at some point\n",
    "2. Calculate the gradient (derivative)\n",
    "3. Move in the opposite direction of the gradient\n",
    "4. Repeat until convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "0d347eb9",
   "metadata": {},
   "source": [
    "# Enhanced gradient descent with visualization\n",
    "def gradient_descent_demo():\n",
    "    # Our cost function: f(x) = (x - 3)^2\n",
    "    def cost_function(x):\n",
    "        return (x - 3)**2\n",
    "\n",
    "    def gradient(x):\n",
    "        return 2 * (x - 3)\n",
    "\n",
    "    # Different learning rates to show their effect\n",
    "    learning_rates = [0.01, 0.1, 0.5]\n",
    "    colors = ['blue', 'red', 'green']\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    for i, lr in enumerate(learning_rates):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "\n",
    "        # Initialize\n",
    "        x = 0.0\n",
    "        history_x = [x]\n",
    "        history_cost = [cost_function(x)]\n",
    "\n",
    "        # Run gradient descent\n",
    "        for iteration in range(20):\n",
    "            grad = gradient(x)\n",
    "            x = x - lr * grad\n",
    "            history_x.append(x)\n",
    "            history_cost.append(cost_function(x))\n",
    "\n",
    "        # Plot the cost function\n",
    "        x_range = np.linspace(-1, 7, 100)\n",
    "        y_range = cost_function(x_range)\n",
    "        plt.plot(x_range, y_range, 'k-', alpha=0.3, label='Cost function')\n",
    "\n",
    "        # Plot the path\n",
    "        plt.plot(history_x, history_cost, 'o-', color=colors[i], \n",
    "                label=f'LR={lr}', markersize=4)\n",
    "        plt.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "        plt.axvline(3, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "        plt.title(f'Learning Rate = {lr}')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Print final results\n",
    "        print(f\"Learning Rate {lr}: Final x = {history_x[-1]:.4f}, Final cost = {history_cost[-1]:.6f}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nKey Observations:\")\n",
    "    print(\"- Too small learning rate (0.01): Slow convergence\")\n",
    "    print(\"- Good learning rate (0.1): Smooth, fast convergence\")\n",
    "    print(\"- Too large learning rate (0.5): May overshoot or oscillate\")\n",
    "\n",
    "gradient_descent_demo()\n",
    "\n",
    "# Show the mathematical intuition\n",
    "print(\"\\n🧠 Mathematical Intuition:\")\n",
    "print(\"- Gradient points uphill (direction of steepest increase)\")\n",
    "print(\"- We want to go downhill, so we move opposite to gradient\")\n",
    "print(\"- Learning rate controls step size\")\n",
    "print(\"- At minimum, gradient = 0, so we stop moving\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "34800fd6",
   "metadata": {},
   "source": [
    "## ✅ Summary Quiz & Checklist\n",
    "\n",
    "### Quiz Questions\n",
    "1. **What does a derivative represent?**\n",
    "   > A derivative represents the instantaneous rate of change of a function at a specific point. Geometrically, it's the slope of the tangent line to the function at that point.\n",
    "\n",
    "2. **What does the gradient tell us in multiple dimensions?**\n",
    "   > The gradient is a vector that points in the direction of steepest increase of a function. In machine learning, we use the negative gradient to find the direction of steepest decrease (toward the minimum).\n",
    "\n",
    "3. **Why is the learning rate important in gradient descent?**\n",
    "   > The learning rate controls how big steps we take toward the minimum. Too small = slow convergence, too large = overshooting or divergence.\n",
    "\n",
    "4. **How does the chain rule apply to neural networks?**\n",
    "   > Neural networks are compositions of functions (layers). The chain rule allows us to compute gradients through these compositions, enabling backpropagation - the algorithm that trains neural networks.\n",
    "\n",
    "### Self-Assessment Checklist\n",
    "Check off each item as you master it:\n",
    "\n",
    "- [ ] I understand how to compute and interpret a derivative\n",
    "- [ ] I understand what a gradient is in multiple dimensions\n",
    "- [ ] I can apply the chain rule to nested functions\n",
    "- [ ] I can visualize and explain gradient descent\n",
    "- [ ] I know how learning rate affects convergence\n",
    "- [ ] I can connect derivatives to machine learning optimization\n",
    "- [ ] I understand why the chain rule is crucial for neural networks\n",
    "\n",
    "### 🔗 Next Steps\n",
    "- Review the [companion theory file](03_calculus_for_ml.md) for deeper mathematical insights\n",
    "- Practice computing derivatives of different functions\n",
    "- Experiment with different learning rates in gradient descent\n",
    "- Think about how these concepts apply to training neural networks\n",
    "\n",
    "### 💡 Key Takeaways\n",
    "- **Derivatives**: Measure how functions change (slopes)\n",
    "- **Chain Rule**: Essential for computing gradients through function compositions\n",
    "- **Gradient Descent**: Uses derivatives to find function minima\n",
    "- **Learning Rate**: Critical hyperparameter that controls optimization speed\n",
    "- **ML Connection**: All modern ML training relies on gradient-based optimization!\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
